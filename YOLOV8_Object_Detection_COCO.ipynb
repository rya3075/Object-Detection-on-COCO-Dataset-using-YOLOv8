{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸ”§ Installing dependencies...\")\n",
        "!pip install ultralytics torch torchvision opencv-python pillow matplotlib seaborn plotly --quiet\n",
        "!pip install roboflow supervision gradio --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "v846MwhlSDdO",
        "outputId": "888f984b-1047-4fba-8eb5-c9b67fc81b30"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Installing dependencies...\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.6/88.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.2/207.2 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m115.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import shutil\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import time\n",
        "from datetime import datetime\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "from google.colab import drive, files\n",
        "import gradio as gr\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import threading\n",
        "import queue"
      ],
      "metadata": {
        "id": "ItC25IjZSFBB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ef2018be-a1a5-42a3-c543-51dd4a76b10a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4rUMenLrSHVq",
        "outputId": "79572fa3-577a-4b79-ee3d-16ce167e0daa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class COCOToYOLO:\n",
        "    def __init__(self, coco_path='/content/drive/MyDrive/coco_dataset'):\n",
        "        self.coco_path = coco_path\n",
        "        self.yolo_path = '/content/yolo_dataset'\n",
        "        self.class_mapping = {}\n",
        "\n",
        "    def setup_directories(self):\n",
        "        \"\"\"Create YOLO dataset structure\"\"\"\n",
        "        dirs = [\n",
        "            f\"{self.yolo_path}/train/images\",\n",
        "            f\"{self.yolo_path}/train/labels\",\n",
        "            f\"{self.yolo_path}/val/images\",\n",
        "            f\"{self.yolo_path}/val/labels\",\n",
        "            f\"{self.yolo_path}/test/images\",\n",
        "            f\"{self.yolo_path}/test/labels\"\n",
        "        ]\n",
        "        for d in dirs:\n",
        "            os.makedirs(d, exist_ok=True)\n",
        "\n",
        "    def load_coco_data(self):\n",
        "        \"\"\"Load COCO dataset from Google Drive\"\"\"\n",
        "        if not os.path.exists(self.coco_path):\n",
        "            print(\"âŒ COCO dataset not found! Please run the dataset downloader first.\")\n",
        "            return False\n",
        "\n",
        "        # Load class mappings\n",
        "        with open(f\"{self.coco_path}/class_mappings.json\", 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Create simplified class mapping (COCO ID -> YOLO class index)\n",
        "        self.class_mapping = {}\n",
        "        class_names = []\n",
        "\n",
        "        for coco_id, info in data['categories'].items():\n",
        "            yolo_idx = info['idx']\n",
        "            self.class_mapping[int(coco_id)] = yolo_idx\n",
        "            class_names.append(info['name'])\n",
        "\n",
        "        self.class_names = class_names\n",
        "        return True\n",
        "\n",
        "    def convert_bbox_to_yolo(self, bbox, img_width, img_height):\n",
        "        \"\"\"Convert COCO normalized center bbox to YOLO format\"\"\"\n",
        "        cx, cy, w, h = bbox\n",
        "        # COCO bbox is already normalized and in center format\n",
        "        return cx, cy, w, h\n",
        "\n",
        "    def convert_dataset(self, train_split=0.7, val_split=0.2):\n",
        "        \"\"\"Convert COCO to YOLO format\"\"\"\n",
        "        print(\"ğŸ”„ Converting COCO dataset to YOLO format...\")\n",
        "\n",
        "        if not self.load_coco_data():\n",
        "            return False\n",
        "\n",
        "        self.setup_directories()\n",
        "\n",
        "        # Get all image files\n",
        "        images_dir = f\"{self.coco_path}/images\"\n",
        "        annotations_dir = f\"{self.coco_path}/annotations\"\n",
        "\n",
        "        image_files = [f for f in os.listdir(images_dir) if f.endswith('.jpg')]\n",
        "        image_files.sort()\n",
        "\n",
        "        # Split dataset\n",
        "        random.shuffle(image_files)\n",
        "        n_total = len(image_files)\n",
        "        n_train = int(n_total * train_split)\n",
        "        n_val = int(n_total * val_split)\n",
        "\n",
        "        train_files = image_files[:n_train]\n",
        "        val_files = image_files[n_train:n_train + n_val]\n",
        "        test_files = image_files[n_train + n_val:]\n",
        "\n",
        "        splits = {\n",
        "            'train': train_files,\n",
        "            'val': val_files,\n",
        "            'test': test_files\n",
        "        }\n",
        "\n",
        "        for split, files in splits.items():\n",
        "            print(f\"Converting {split}: {len(files)} images\")\n",
        "\n",
        "            for img_file in files:\n",
        "                # Copy image\n",
        "                src_img = f\"{images_dir}/{img_file}\"\n",
        "                dst_img = f\"{self.yolo_path}/{split}/images/{img_file}\"\n",
        "                shutil.copy2(src_img, dst_img)\n",
        "\n",
        "                # Convert annotation\n",
        "                anno_file = img_file.replace('.jpg', '.json')\n",
        "                anno_path = f\"{annotations_dir}/{anno_file}\"\n",
        "\n",
        "                if os.path.exists(anno_path):\n",
        "                    with open(anno_path, 'r') as f:\n",
        "                        anno_data = json.load(f)\n",
        "\n",
        "                    # Create YOLO label file\n",
        "                    label_file = img_file.replace('.jpg', '.txt')\n",
        "                    label_path = f\"{self.yolo_path}/{split}/labels/{label_file}\"\n",
        "\n",
        "                    with open(label_path, 'w') as f:\n",
        "                        for obj in anno_data['objects']:\n",
        "                            coco_id = obj['category_id']\n",
        "                            if coco_id in self.class_mapping:\n",
        "                                yolo_class = self.class_mapping[coco_id]\n",
        "                                cx, cy, w, h = self.convert_bbox_to_yolo(\n",
        "                                    obj['bbox'], anno_data['width'], anno_data['height']\n",
        "                                )\n",
        "                                f.write(f\"{yolo_class} {cx:.6f} {cy:.6f} {w:.6f} {h:.6f}\\n\")\n",
        "\n",
        "        # Create dataset.yaml\n",
        "        self.create_yaml()\n",
        "\n",
        "        print(\"âœ… Dataset conversion completed!\")\n",
        "        return True\n",
        "\n",
        "    def create_yaml(self):\n",
        "        \"\"\"Create dataset.yaml for YOLO training\"\"\"\n",
        "        yaml_content = {\n",
        "            'path': self.yolo_path,\n",
        "            'train': 'train/images',\n",
        "            'val': 'val/images',\n",
        "            'test': 'test/images',\n",
        "            'nc': len(self.class_names),\n",
        "            'names': self.class_names\n",
        "        }\n",
        "\n",
        "        with open(f\"{self.yolo_path}/dataset.yaml\", 'w') as f:\n",
        "            yaml.dump(yaml_content, f)\n",
        "\n",
        "        print(f\"ğŸ“„ Created dataset.yaml with {len(self.class_names)} classes\")\n",
        "\n",
        "# Initialize and convert dataset\n",
        "converter = COCOToYOLO()\n",
        "converter.convert_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "IocuOJraSOOP",
        "outputId": "78e094a2-7c2d-45d2-b290-6d18b0e0a69d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Converting COCO dataset to YOLO format...\n",
            "Converting train: 700 images\n",
            "Converting val: 200 images\n",
            "Converting test: 100 images\n",
            "ğŸ“„ Created dataset.yaml with 80 classes\n",
            "âœ… Dataset conversion completed!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class YOLOv8Trainer:\n",
        "    def __init__(self, dataset_path='/content/yolo_dataset'):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.model_path = '/content/best_model.pt'\n",
        "        self.results_path = '/content/training_results'\n",
        "        os.makedirs(self.results_path, exist_ok=True)\n",
        "\n",
        "    def train_model(self, epochs=50, img_size=640, batch_size=16, device='cuda'):\n",
        "        \"\"\"Train YOLOv8 model\"\"\"\n",
        "        print(\"ğŸš€ Starting YOLOv8 training...\")\n",
        "\n",
        "        # Initialize model\n",
        "        model = YOLO('yolov8n.pt')  # nano version for faster training\n",
        "\n",
        "        # Train\n",
        "        results = model.train(\n",
        "            data=f'{self.dataset_path}/dataset.yaml',\n",
        "            epochs=epochs,\n",
        "            imgsz=img_size,\n",
        "            batch=batch_size,\n",
        "            device=device,\n",
        "            project=self.results_path,\n",
        "            name='training',\n",
        "            save=True,\n",
        "            plots=True,\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        # Save best model\n",
        "        best_model_source = f\"{self.results_path}/training/weights/best.pt\"\n",
        "        if os.path.exists(best_model_source):\n",
        "            shutil.copy2(best_model_source, self.model_path)\n",
        "            print(f\"âœ… Best model saved to: {self.model_path}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def validate_model(self):\n",
        "        \"\"\"Validate trained model\"\"\"\n",
        "        if not os.path.exists(self.model_path):\n",
        "            print(\"âŒ No trained model found!\")\n",
        "            return None\n",
        "\n",
        "        model = YOLO(self.model_path)\n",
        "        results = model.val(data=f'{self.dataset_path}/dataset.yaml')\n",
        "        return results\n",
        "\n",
        "# Train the model\n",
        "trainer = YOLOv8Trainer()\n",
        "training_results = trainer.train_model(epochs=30)  # Reduced for demo\n",
        "validation_results = trainer.validate_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qu-wycS3SRfe",
        "outputId": "ccc9995d-aa2c-4b3c-939f-d5a8e2708c3a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Starting YOLOv8 training...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 6.2MB 16.7MB/s 0.4s\n",
            "Ultralytics 8.3.194 ğŸš€ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/yolo_dataset/dataset.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=training, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/training_results, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/training_results/training, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 77.5MB/s 0.0s\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
            "Model summary: 129 layers, 3,157,200 parameters, 3,157,184 gradients, 8.9 GFLOPs\n",
            "\n",
            "Transferred 355/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.4MB 223.8MB/s 0.0s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1911.6Â±675.4 MB/s, size: 121.9 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/yolo_dataset/train/labels... 700 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 700/700 2.0Kit/s 0.4s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/yolo_dataset/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1035.6Â±532.4 MB/s, size: 128.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/yolo_dataset/val/labels... 200 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 200/200 1.5Kit/s 0.1s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/yolo_dataset/val/labels.cache\n",
            "Plotting labels to /content/training_results/training/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/training_results/training\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/30      2.58G      1.204      1.493      1.265        129        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 2.8it/s 15.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 2.5it/s 2.9s\n",
            "                   all        200       1521      0.613      0.501      0.564      0.421\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/30      3.37G      1.194      1.431      1.246        193        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.4it/s 12.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 4.0it/s 1.8s\n",
            "                   all        200       1521      0.583      0.503      0.551      0.405\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/30      3.39G      1.168       1.36      1.215        192        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.5it/s 12.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 4.2it/s 1.7s\n",
            "                   all        200       1521      0.599      0.477      0.546        0.4\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/30      3.41G      1.161      1.338      1.202        213        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.6it/s 12.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 4.2it/s 1.7s\n",
            "                   all        200       1521      0.621       0.45       0.53      0.395\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/30      3.43G      1.143      1.333      1.211         98        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.5it/s 12.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.8it/s 1.8s\n",
            "                   all        200       1521      0.635      0.429      0.514       0.38\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/30      3.45G      1.165      1.316       1.22        168        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.5it/s 12.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 4.1it/s 1.7s\n",
            "                   all        200       1521      0.561      0.436      0.511      0.382\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/30      3.46G      1.135      1.271      1.185        115        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.5it/s 12.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 4.7it/s 1.5s\n",
            "                   all        200       1521       0.64      0.449      0.519      0.384\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/30      3.48G      1.131      1.264      1.205        250        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.5it/s 12.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 4.3it/s 1.6s\n",
            "                   all        200       1521      0.539      0.443      0.514       0.38\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/30       3.5G      1.119       1.23      1.183        167        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.5it/s 12.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 4.0it/s 1.7s\n",
            "                   all        200       1521      0.552      0.445      0.516      0.377\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/30      3.52G      1.118       1.22      1.184        144        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.5it/s 12.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 4.4it/s 1.6s\n",
            "                   all        200       1521      0.563      0.451      0.508      0.376\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/30      3.54G      1.095      1.192      1.178        179        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.6it/s 12.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 4.1it/s 1.7s\n",
            "                   all        200       1521      0.607      0.441      0.514      0.374\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/30      3.55G      1.122      1.202      1.197        133        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.5it/s 12.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 4.3it/s 1.6s\n",
            "                   all        200       1521      0.556      0.461      0.518      0.368\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/30      3.57G      1.089      1.144      1.164        155        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.6it/s 12.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 4.3it/s 1.6s\n",
            "                   all        200       1521      0.617      0.423      0.512      0.374\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/30      3.59G      1.092       1.15      1.168        155        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.4it/s 12.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 4.6it/s 1.5s\n",
            "                   all        200       1521      0.497      0.474      0.507      0.375\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/30      3.61G      1.088      1.118      1.164        128        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.5it/s 12.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 4.3it/s 1.6s\n",
            "                   all        200       1521      0.562      0.442      0.499      0.369\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/30      3.62G      1.077      1.104      1.161        265        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.5it/s 12.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 4.3it/s 1.6s\n",
            "                   all        200       1521      0.519      0.465      0.501      0.368\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/30      3.64G      1.084      1.112      1.163        200        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.5it/s 12.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 4.1it/s 1.7s\n",
            "                   all        200       1521      0.513      0.455      0.501       0.37\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/30      3.66G      1.031      1.071      1.146        140        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.5it/s 12.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 4.0it/s 1.7s\n",
            "                   all        200       1521      0.536      0.455      0.505       0.37\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/30      3.68G       1.06      1.094      1.154        228        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.5it/s 12.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 4.0it/s 1.8s\n",
            "                   all        200       1521      0.553      0.433      0.496      0.364\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/30       3.7G      1.066      1.093       1.16        177        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.4it/s 12.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 4.4it/s 1.6s\n",
            "                   all        200       1521      0.529      0.441      0.498      0.364\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/30      3.71G      1.073       1.13      1.147         54        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.2it/s 13.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.2it/s 2.2s\n",
            "                   all        200       1521      0.562      0.435      0.485      0.355\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/30      3.73G      1.055      1.067      1.141        101        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.9it/s 11.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.3it/s 2.1s\n",
            "                   all        200       1521      0.544      0.445      0.491      0.354\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/30      3.75G      1.018     0.9971      1.119         63        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 4.0it/s 11.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 3.8it/s 1.8s\n",
            "                   all        200       1521      0.518      0.454       0.49      0.353\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/30      3.77G      1.012     0.9843       1.11         71        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.8it/s 11.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 4.2it/s 1.7s\n",
            "                   all        200       1521      0.529      0.461       0.49      0.352\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/30      3.79G      1.001     0.9709      1.109        113        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.8it/s 11.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 4.3it/s 1.6s\n",
            "                   all        200       1521      0.555      0.443       0.49      0.355\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      26/30       3.8G      1.016       0.97      1.105         78        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.7it/s 11.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 4.4it/s 1.6s\n",
            "                   all        200       1521      0.522      0.458      0.491      0.353\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      27/30      3.82G     0.9949     0.9548      1.102         60        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.7it/s 11.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 4.4it/s 1.6s\n",
            "                   all        200       1521      0.535      0.455       0.49      0.356\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      28/30      3.84G     0.9861     0.9383      1.102         78        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.7it/s 11.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 4.1it/s 1.7s\n",
            "                   all        200       1521      0.487      0.476      0.486      0.353\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      29/30      3.86G     0.9914     0.9272      1.101         93        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.6it/s 12.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 4.1it/s 1.7s\n",
            "                   all        200       1521      0.517      0.462      0.485      0.352\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      30/30      3.87G      1.005     0.9291      1.109        113        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 44/44 3.6it/s 12.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 4.4it/s 1.6s\n",
            "                   all        200       1521      0.511      0.463      0.487      0.354\n",
            "\n",
            "30 epochs completed in 0.122 hours.\n",
            "Optimizer stripped from /content/training_results/training/weights/last.pt, 6.5MB\n",
            "Optimizer stripped from /content/training_results/training/weights/best.pt, 6.5MB\n",
            "\n",
            "Validating /content/training_results/training/weights/best.pt...\n",
            "Ultralytics 8.3.194 ğŸš€ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 1.6it/s 4.3s\n",
            "                   all        200       1521      0.617      0.501      0.564      0.422\n",
            "                person        116        402      0.816      0.684      0.785      0.541\n",
            "               bicycle          6          6          1      0.487      0.704      0.315\n",
            "                   car         30         86      0.672      0.442      0.523      0.312\n",
            "            motorcycle          2          3      0.522      0.667      0.679      0.551\n",
            "              airplane          2          6      0.515      0.333      0.559      0.311\n",
            "                   bus          6          6      0.512      0.667      0.782      0.663\n",
            "                 train          6          6      0.922      0.833      0.931      0.842\n",
            "                 truck         13         23      0.608      0.304      0.459      0.342\n",
            "                  boat          3         14       0.84      0.214       0.33      0.107\n",
            "         traffic light          6         18      0.659      0.333      0.369      0.228\n",
            "          fire hydrant          6          7      0.882      0.714      0.769      0.606\n",
            "             stop sign          4          4      0.868          1      0.995      0.921\n",
            "         parking meter          1          1      0.305          1      0.995      0.995\n",
            "                 bench         12         28      0.778      0.107      0.137      0.117\n",
            "                  bird          4         21      0.581      0.238      0.358       0.22\n",
            "                   cat          6          6       0.56      0.667      0.636       0.57\n",
            "                   dog          2          2      0.964          1      0.995      0.812\n",
            "                 horse          4         10          1      0.621      0.788      0.488\n",
            "                 sheep          3         17      0.864      0.765      0.867      0.625\n",
            "                   cow          6         31      0.571      0.677      0.709      0.479\n",
            "              elephant          2          3      0.897          1      0.995      0.831\n",
            "                  bear          2          2      0.482        0.5      0.572      0.572\n",
            "                 zebra          2          3      0.771      0.667      0.913       0.79\n",
            "               giraffe          5         14          1      0.997      0.995      0.804\n",
            "              backpack         13         29      0.475      0.157      0.236      0.115\n",
            "              umbrella         10         18      0.541      0.667      0.684      0.437\n",
            "               handbag         18         32      0.669      0.219      0.271      0.129\n",
            "                   tie          9         17      0.849      0.333      0.518      0.264\n",
            "              suitcase          6         15       0.58      0.461      0.526      0.319\n",
            "               frisbee          3          3      0.794          1      0.995       0.83\n",
            "                  skis          8         12      0.631      0.429      0.481      0.297\n",
            "             snowboard          2          2       0.29        0.5      0.448      0.274\n",
            "           sports ball          8          9      0.552      0.556      0.549      0.444\n",
            "          baseball bat          7          9      0.488      0.333      0.475      0.234\n",
            "        baseball glove          7         13      0.784      0.561      0.653      0.457\n",
            "            skateboard          4          5      0.467        0.6      0.529      0.295\n",
            "             surfboard          6         21      0.624      0.381      0.395      0.225\n",
            "         tennis racket          3          7       0.51      0.286      0.514      0.372\n",
            "                bottle         19         35      0.706      0.486      0.547      0.388\n",
            "            wine glass          3          8      0.204      0.125      0.119     0.0282\n",
            "                   cup         13         43       0.53      0.326      0.389      0.261\n",
            "                  fork          5          9        0.7      0.556      0.625      0.466\n",
            "                 knife          9         23      0.398      0.087      0.172      0.111\n",
            "                 spoon          6          6          0          0      0.181      0.137\n",
            "                  bowl         10         17      0.436      0.412      0.443      0.359\n",
            "                banana          4          7      0.231      0.132      0.192      0.138\n",
            "                 apple          3         13      0.363      0.615      0.462      0.376\n",
            "              sandwich          3          6      0.821        0.5        0.6      0.543\n",
            "                orange          4          4      0.314        0.5      0.561      0.515\n",
            "              broccoli          2         18      0.216      0.123        0.2     0.0794\n",
            "                carrot          9         28      0.269      0.214      0.165     0.0942\n",
            "               hot dog          3          3          1          0      0.128     0.0935\n",
            "                 pizza          4          8      0.452      0.625      0.691      0.582\n",
            "                 donut          2          2       0.58        0.5      0.662      0.546\n",
            "                  cake          4         18      0.463      0.241      0.238      0.199\n",
            "                 chair         21         89      0.599      0.348      0.443      0.257\n",
            "                 couch          7         11      0.856      0.541      0.568      0.469\n",
            "          potted plant         10         22      0.623      0.318      0.427      0.304\n",
            "                   bed          7          7      0.543      0.714       0.69       0.57\n",
            "          dining table         22         32      0.486      0.375      0.471      0.331\n",
            "                toilet          3          3      0.692          1      0.995      0.962\n",
            "                    tv          9         12      0.506      0.667      0.755      0.554\n",
            "                laptop          9         20       0.54       0.55      0.547      0.429\n",
            "                 mouse          8          9      0.562      0.778      0.858      0.594\n",
            "                remote          4         12      0.374      0.203       0.34      0.154\n",
            "              keyboard          7         13      0.689      0.462       0.59      0.433\n",
            "            cell phone         10         11      0.592      0.727      0.657      0.537\n",
            "             microwave          4          4       0.54        0.5      0.722      0.444\n",
            "                  oven          4          4      0.506       0.75      0.746      0.696\n",
            "                  sink          9         12      0.463       0.25      0.272      0.161\n",
            "          refrigerator          7         23      0.574      0.304       0.41      0.257\n",
            "                  book          6         21      0.792      0.183      0.281      0.142\n",
            "                 clock          9         16      0.809      0.938      0.959       0.76\n",
            "                  vase          8         21      0.783      0.571      0.663      0.448\n",
            "              scissors          2          2      0.873          1      0.995      0.721\n",
            "            teddy bear          4         16      0.572     0.0895      0.131      0.111\n",
            "            hair drier          1          1          1          0          0          0\n",
            "            toothbrush          1          1      0.641          1      0.995      0.895\n",
            "Speed: 0.3ms preprocess, 2.7ms inference, 0.0ms loss, 5.5ms postprocess per image\n",
            "Results saved to \u001b[1m/content/training_results/training\u001b[0m\n",
            "âœ… Best model saved to: /content/best_model.pt\n",
            "Ultralytics 8.3.194 ğŸš€ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2008.2Â±723.8 MB/s, size: 144.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/yolo_dataset/val/labels.cache... 200 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 200/200 433.5Kit/s 0.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.9it/s 3.3s\n",
            "                   all        200       1521      0.603      0.504      0.563      0.421\n",
            "                person        116        402      0.806      0.684      0.785       0.54\n",
            "               bicycle          6          6      0.744      0.488      0.694      0.318\n",
            "                   car         30         86      0.664      0.442      0.526      0.313\n",
            "            motorcycle          2          3      0.517      0.667      0.679      0.551\n",
            "              airplane          2          6      0.413      0.333      0.533      0.294\n",
            "                   bus          6          6      0.481      0.667      0.782      0.663\n",
            "                 train          6          6      0.915      0.833      0.931      0.842\n",
            "                 truck         13         23      0.589      0.304      0.458      0.341\n",
            "                  boat          3         14      0.799      0.214      0.329      0.105\n",
            "         traffic light          6         18      0.651      0.333      0.369      0.228\n",
            "          fire hydrant          6          7       0.88      0.714      0.768      0.605\n",
            "             stop sign          4          4      0.865          1      0.995      0.921\n",
            "         parking meter          1          1      0.293          1      0.995      0.995\n",
            "                 bench         12         28       0.77      0.107      0.137      0.117\n",
            "                  bird          4         21      0.551      0.238      0.371      0.221\n",
            "                   cat          6          6      0.551      0.667      0.636      0.564\n",
            "                   dog          2          2      0.823          1      0.995      0.829\n",
            "                 horse          4         10          1      0.624      0.788      0.479\n",
            "                 sheep          3         17      0.851      0.765      0.867      0.624\n",
            "                   cow          6         31      0.562      0.677      0.709      0.485\n",
            "              elephant          2          3      0.892          1      0.995      0.831\n",
            "                  bear          2          2      0.476        0.5      0.572      0.572\n",
            "                 zebra          2          3      0.765      0.667      0.913       0.79\n",
            "               giraffe          5         14      0.998          1      0.995       0.81\n",
            "              backpack         13         29      0.488      0.165      0.236      0.115\n",
            "              umbrella         10         18      0.531      0.667      0.682      0.436\n",
            "               handbag         18         32      0.658      0.219      0.283      0.146\n",
            "                   tie          9         17      0.852      0.341      0.516      0.263\n",
            "              suitcase          6         15      0.583      0.466      0.521      0.315\n",
            "               frisbee          3          3      0.796          1      0.995       0.83\n",
            "                  skis          8         12      0.633      0.433      0.481      0.293\n",
            "             snowboard          2          2      0.264        0.5      0.414      0.308\n",
            "           sports ball          8          9      0.548      0.556      0.548      0.474\n",
            "          baseball bat          7          9       0.48      0.333      0.475      0.234\n",
            "        baseball glove          7         13      0.787      0.569      0.653      0.456\n",
            "            skateboard          4          5      0.463        0.6      0.527      0.294\n",
            "             surfboard          6         21      0.642      0.429      0.389      0.225\n",
            "         tennis racket          3          7      0.498      0.286      0.514      0.372\n",
            "                bottle         19         35      0.715      0.501      0.547      0.382\n",
            "            wine glass          3          8       0.21      0.135       0.12     0.0276\n",
            "                   cup         13         43      0.526      0.326      0.391      0.261\n",
            "                  fork          5          9      0.693      0.556      0.602       0.46\n",
            "                 knife          9         23      0.358      0.087      0.169      0.111\n",
            "                 spoon          6          6          0          0      0.174      0.124\n",
            "                  bowl         10         17      0.406      0.412      0.441      0.358\n",
            "                banana          4          7      0.239      0.143      0.195      0.139\n",
            "                 apple          3         13      0.358      0.615       0.46      0.376\n",
            "              sandwich          3          6      0.817        0.5      0.601      0.524\n",
            "                orange          4          4      0.311        0.5      0.558      0.515\n",
            "              broccoli          2         18      0.252      0.151      0.212     0.0694\n",
            "                carrot          9         28      0.223      0.214      0.159     0.0933\n",
            "               hot dog          3          3          1          0      0.124     0.0876\n",
            "                 pizza          4          8      0.488      0.625      0.691      0.562\n",
            "                 donut          2          2      0.571        0.5      0.662      0.546\n",
            "                  cake          4         18      0.475      0.253      0.238      0.193\n",
            "                 chair         21         89       0.61      0.369      0.442      0.258\n",
            "                 couch          7         11      0.857      0.543      0.568      0.469\n",
            "          potted plant         10         22      0.651       0.34      0.456      0.325\n",
            "                   bed          7          7      0.491      0.714      0.677      0.554\n",
            "          dining table         22         32      0.484      0.382      0.476       0.33\n",
            "                toilet          3          3      0.688          1      0.995      0.951\n",
            "                    tv          9         12      0.475      0.667      0.755      0.553\n",
            "                laptop          9         20      0.536       0.55      0.547      0.429\n",
            "                 mouse          8          9      0.561      0.778      0.847      0.589\n",
            "                remote          4         12      0.381      0.208      0.338      0.162\n",
            "              keyboard          7         13      0.682      0.462      0.596      0.437\n",
            "            cell phone         10         11      0.588      0.727      0.656      0.532\n",
            "             microwave          4          4      0.514        0.5      0.716      0.472\n",
            "                  oven          4          4      0.499       0.75      0.746      0.696\n",
            "                  sink          9         12      0.436       0.25      0.268      0.159\n",
            "          refrigerator          7         23       0.58      0.261       0.42      0.253\n",
            "                  book          6         21      0.658      0.184       0.27      0.136\n",
            "                 clock          9         16      0.813      0.938      0.955      0.737\n",
            "                  vase          8         21      0.744      0.571      0.656      0.441\n",
            "              scissors          2          2      0.866          1      0.995      0.721\n",
            "            teddy bear          4         16      0.576     0.0909      0.131      0.111\n",
            "            hair drier          1          1          1          0          0          0\n",
            "            toothbrush          1          1      0.626          1      0.995      0.895\n",
            "Speed: 0.7ms preprocess, 5.3ms inference, 0.0ms loss, 2.2ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/val\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MetricsDashboard:\n",
        "    def __init__(self, model_path='/content/best_model.pt'):\n",
        "        self.model = YOLO(model_path) if os.path.exists(model_path) else None\n",
        "        self.metrics_history = []\n",
        "\n",
        "    def calculate_metrics(self, test_images_path='/content/yolo_dataset/test/images'):\n",
        "        \"\"\"Calculate comprehensive metrics\"\"\"\n",
        "        if not self.model:\n",
        "            return None\n",
        "\n",
        "        # Run validation to get metrics\n",
        "        results = self.model.val(data='/content/yolo_dataset/dataset.yaml')\n",
        "\n",
        "        metrics = {\n",
        "            'mAP50': float(results.box.map50),\n",
        "            'mAP50-95': float(results.box.map),\n",
        "            'precision': float(results.box.mp),\n",
        "            'recall': float(results.box.mr),\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        self.metrics_history.append(metrics)\n",
        "        return metrics\n",
        "\n",
        "    def create_dashboard(self):\n",
        "        \"\"\"Create comprehensive metrics dashboard\"\"\"\n",
        "        if not self.model:\n",
        "            print(\"âŒ No model available for dashboard\")\n",
        "            return\n",
        "\n",
        "        # Calculate current metrics\n",
        "        current_metrics = self.calculate_metrics()\n",
        "\n",
        "        # Create subplots\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=('Model Performance Metrics', 'Precision vs Recall',\n",
        "                          'Confidence Distribution', 'Class Performance'),\n",
        "            specs=[[{\"type\": \"indicator\"}, {\"type\": \"scatter\"}],\n",
        "                   [{\"type\": \"histogram\"}, {\"type\": \"bar\"}]]\n",
        "        )\n",
        "\n",
        "        # Metric gauges\n",
        "        metrics = ['mAP50', 'mAP50-95', 'Precision', 'Recall']\n",
        "        values = [current_metrics['mAP50'], current_metrics['mAP50-95'],\n",
        "                 current_metrics['precision'], current_metrics['recall']]\n",
        "\n",
        "        colors = ['blue', 'green', 'orange', 'red']\n",
        "\n",
        "        for i, (metric, value, color) in enumerate(zip(metrics, values, colors)):\n",
        "            fig.add_trace(go.Indicator(\n",
        "                mode=\"gauge+number+delta\",\n",
        "                value=value * 100,\n",
        "                domain={'row': 0, 'column': 0},\n",
        "                title={'text': metric},\n",
        "                gauge={'axis': {'range': [None, 100]},\n",
        "                       'bar': {'color': color},\n",
        "                       'steps': [{'range': [0, 50], 'color': \"lightgray\"},\n",
        "                                {'range': [50, 80], 'color': \"gray\"},\n",
        "                                {'range': [80, 100], 'color': \"lightgreen\"}],\n",
        "                       'threshold': {'line': {'color': \"red\", 'width': 4},\n",
        "                                   'thickness': 0.75, 'value': 90}}\n",
        "            ), row=1, col=1)\n",
        "\n",
        "        # Precision vs Recall scatter\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=[current_metrics['recall']],\n",
        "            y=[current_metrics['precision']],\n",
        "            mode='markers+text',\n",
        "            marker=dict(size=15, color='red'),\n",
        "            text=['Current Model'],\n",
        "            textposition=\"top center\",\n",
        "            name='Model Performance'\n",
        "        ), row=1, col=2)\n",
        "\n",
        "        # Update layout\n",
        "        fig.update_layout(\n",
        "            title_text=\"ğŸ¯ YOLOv8 Performance Dashboard\",\n",
        "            showlegend=True,\n",
        "            height=800\n",
        "        )\n",
        "\n",
        "        fig.show()\n",
        "\n",
        "        # Print detailed metrics\n",
        "        print(\"\\nğŸ“Š DETAILED PERFORMANCE METRICS\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"ğŸ¯ mAP@50: {current_metrics['mAP50']:.3f}\")\n",
        "        print(f\"ğŸ¯ mAP@50-95: {current_metrics['mAP50-95']:.3f}\")\n",
        "        print(f\"ğŸ“ˆ Precision: {current_metrics['precision']:.3f}\")\n",
        "        print(f\"ğŸ“ˆ Recall: {current_metrics['recall']:.3f}\")\n",
        "        print(f\"â° Timestamp: {current_metrics['timestamp']}\")\n",
        "\n",
        "# Create and display dashboard\n",
        "dashboard = MetricsDashboard()\n",
        "dashboard.create_dashboard()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2448
        },
        "id": "ktodOL24SXUq",
        "outputId": "5ec1d634-c0dd-4af5-c720-2ca87edcab21"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.194 ğŸš€ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2690.6Â±437.1 MB/s, size: 135.0 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/yolo_dataset/val/labels.cache... 200 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 200/200 419.0Kit/s 0.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.8it/s 3.5s\n",
            "                   all        200       1521      0.603      0.504      0.563      0.421\n",
            "                person        116        402      0.806      0.684      0.785       0.54\n",
            "               bicycle          6          6      0.744      0.488      0.694      0.318\n",
            "                   car         30         86      0.664      0.442      0.526      0.313\n",
            "            motorcycle          2          3      0.517      0.667      0.679      0.551\n",
            "              airplane          2          6      0.413      0.333      0.533      0.294\n",
            "                   bus          6          6      0.481      0.667      0.782      0.663\n",
            "                 train          6          6      0.915      0.833      0.931      0.842\n",
            "                 truck         13         23      0.589      0.304      0.458      0.341\n",
            "                  boat          3         14      0.799      0.214      0.329      0.105\n",
            "         traffic light          6         18      0.651      0.333      0.369      0.228\n",
            "          fire hydrant          6          7       0.88      0.714      0.768      0.605\n",
            "             stop sign          4          4      0.865          1      0.995      0.921\n",
            "         parking meter          1          1      0.293          1      0.995      0.995\n",
            "                 bench         12         28       0.77      0.107      0.137      0.117\n",
            "                  bird          4         21      0.551      0.238      0.371      0.221\n",
            "                   cat          6          6      0.551      0.667      0.636      0.564\n",
            "                   dog          2          2      0.823          1      0.995      0.829\n",
            "                 horse          4         10          1      0.624      0.788      0.479\n",
            "                 sheep          3         17      0.851      0.765      0.867      0.624\n",
            "                   cow          6         31      0.562      0.677      0.709      0.485\n",
            "              elephant          2          3      0.892          1      0.995      0.831\n",
            "                  bear          2          2      0.476        0.5      0.572      0.572\n",
            "                 zebra          2          3      0.765      0.667      0.913       0.79\n",
            "               giraffe          5         14      0.998          1      0.995       0.81\n",
            "              backpack         13         29      0.488      0.165      0.236      0.115\n",
            "              umbrella         10         18      0.531      0.667      0.682      0.436\n",
            "               handbag         18         32      0.658      0.219      0.283      0.146\n",
            "                   tie          9         17      0.852      0.341      0.516      0.263\n",
            "              suitcase          6         15      0.583      0.466      0.521      0.315\n",
            "               frisbee          3          3      0.796          1      0.995       0.83\n",
            "                  skis          8         12      0.633      0.433      0.481      0.293\n",
            "             snowboard          2          2      0.264        0.5      0.414      0.308\n",
            "           sports ball          8          9      0.548      0.556      0.548      0.474\n",
            "          baseball bat          7          9       0.48      0.333      0.475      0.234\n",
            "        baseball glove          7         13      0.787      0.569      0.653      0.456\n",
            "            skateboard          4          5      0.463        0.6      0.527      0.294\n",
            "             surfboard          6         21      0.642      0.429      0.389      0.225\n",
            "         tennis racket          3          7      0.498      0.286      0.514      0.372\n",
            "                bottle         19         35      0.715      0.501      0.547      0.382\n",
            "            wine glass          3          8       0.21      0.135       0.12     0.0276\n",
            "                   cup         13         43      0.526      0.326      0.391      0.261\n",
            "                  fork          5          9      0.693      0.556      0.602       0.46\n",
            "                 knife          9         23      0.358      0.087      0.169      0.111\n",
            "                 spoon          6          6          0          0      0.174      0.124\n",
            "                  bowl         10         17      0.406      0.412      0.441      0.358\n",
            "                banana          4          7      0.239      0.143      0.195      0.139\n",
            "                 apple          3         13      0.358      0.615       0.46      0.376\n",
            "              sandwich          3          6      0.817        0.5      0.601      0.524\n",
            "                orange          4          4      0.311        0.5      0.558      0.515\n",
            "              broccoli          2         18      0.252      0.151      0.212     0.0694\n",
            "                carrot          9         28      0.223      0.214      0.159     0.0933\n",
            "               hot dog          3          3          1          0      0.124     0.0876\n",
            "                 pizza          4          8      0.488      0.625      0.691      0.562\n",
            "                 donut          2          2      0.571        0.5      0.662      0.546\n",
            "                  cake          4         18      0.475      0.253      0.238      0.193\n",
            "                 chair         21         89       0.61      0.369      0.442      0.258\n",
            "                 couch          7         11      0.857      0.543      0.568      0.469\n",
            "          potted plant         10         22      0.651       0.34      0.456      0.325\n",
            "                   bed          7          7      0.491      0.714      0.677      0.554\n",
            "          dining table         22         32      0.484      0.382      0.476       0.33\n",
            "                toilet          3          3      0.688          1      0.995      0.951\n",
            "                    tv          9         12      0.475      0.667      0.755      0.553\n",
            "                laptop          9         20      0.536       0.55      0.547      0.429\n",
            "                 mouse          8          9      0.561      0.778      0.847      0.589\n",
            "                remote          4         12      0.381      0.208      0.338      0.162\n",
            "              keyboard          7         13      0.682      0.462      0.596      0.437\n",
            "            cell phone         10         11      0.588      0.727      0.656      0.532\n",
            "             microwave          4          4      0.514        0.5      0.716      0.472\n",
            "                  oven          4          4      0.499       0.75      0.746      0.696\n",
            "                  sink          9         12      0.436       0.25      0.268      0.159\n",
            "          refrigerator          7         23       0.58      0.261       0.42      0.253\n",
            "                  book          6         21      0.658      0.184       0.27      0.136\n",
            "                 clock          9         16      0.813      0.938      0.955      0.737\n",
            "                  vase          8         21      0.744      0.571      0.656      0.441\n",
            "              scissors          2          2      0.866          1      0.995      0.721\n",
            "            teddy bear          4         16      0.576     0.0909      0.131      0.111\n",
            "            hair drier          1          1          1          0          0          0\n",
            "            toothbrush          1          1      0.626          1      0.995      0.895\n",
            "Speed: 1.4ms preprocess, 4.5ms inference, 0.0ms loss, 1.6ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/val2\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"080d3f86-9dc1-4541-8b2d-3e506be20a1b\" class=\"plotly-graph-div\" style=\"height:800px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"080d3f86-9dc1-4541-8b2d-3e506be20a1b\")) {                    Plotly.newPlot(                        \"080d3f86-9dc1-4541-8b2d-3e506be20a1b\",                        [{\"domain\":{\"column\":0,\"row\":0,\"x\":[0.0,0.45],\"y\":[0.625,1.0]},\"gauge\":{\"axis\":{\"range\":[null,100]},\"bar\":{\"color\":\"blue\"},\"steps\":[{\"color\":\"lightgray\",\"range\":[0,50]},{\"color\":\"gray\",\"range\":[50,80]},{\"color\":\"lightgreen\",\"range\":[80,100]}],\"threshold\":{\"line\":{\"color\":\"red\",\"width\":4},\"thickness\":0.75,\"value\":90}},\"mode\":\"gauge+number+delta\",\"title\":{\"text\":\"mAP50\"},\"value\":56.29163750979773,\"type\":\"indicator\"},{\"domain\":{\"column\":0,\"row\":0,\"x\":[0.0,0.45],\"y\":[0.625,1.0]},\"gauge\":{\"axis\":{\"range\":[null,100]},\"bar\":{\"color\":\"green\"},\"steps\":[{\"color\":\"lightgray\",\"range\":[0,50]},{\"color\":\"gray\",\"range\":[50,80]},{\"color\":\"lightgreen\",\"range\":[80,100]}],\"threshold\":{\"line\":{\"color\":\"red\",\"width\":4},\"thickness\":0.75,\"value\":90}},\"mode\":\"gauge+number+delta\",\"title\":{\"text\":\"mAP50-95\"},\"value\":42.105058331056284,\"type\":\"indicator\"},{\"domain\":{\"column\":0,\"row\":0,\"x\":[0.0,0.45],\"y\":[0.625,1.0]},\"gauge\":{\"axis\":{\"range\":[null,100]},\"bar\":{\"color\":\"orange\"},\"steps\":[{\"color\":\"lightgray\",\"range\":[0,50]},{\"color\":\"gray\",\"range\":[50,80]},{\"color\":\"lightgreen\",\"range\":[80,100]}],\"threshold\":{\"line\":{\"color\":\"red\",\"width\":4},\"thickness\":0.75,\"value\":90}},\"mode\":\"gauge+number+delta\",\"title\":{\"text\":\"Precision\"},\"value\":60.30799910256193,\"type\":\"indicator\"},{\"domain\":{\"column\":0,\"row\":0,\"x\":[0.0,0.45],\"y\":[0.625,1.0]},\"gauge\":{\"axis\":{\"range\":[null,100]},\"bar\":{\"color\":\"red\"},\"steps\":[{\"color\":\"lightgray\",\"range\":[0,50]},{\"color\":\"gray\",\"range\":[50,80]},{\"color\":\"lightgreen\",\"range\":[80,100]}],\"threshold\":{\"line\":{\"color\":\"red\",\"width\":4},\"thickness\":0.75,\"value\":90}},\"mode\":\"gauge+number+delta\",\"title\":{\"text\":\"Recall\"},\"value\":50.37094710898079,\"type\":\"indicator\"},{\"marker\":{\"color\":\"red\",\"size\":15},\"mode\":\"markers+text\",\"name\":\"Model Performance\",\"text\":[\"Current Model\"],\"textposition\":\"top center\",\"x\":[0.5037094710898079],\"y\":[0.6030799910256193],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.55,1.0]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.625,1.0]},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.0,0.45]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,0.375]},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.55,1.0]},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.0,0.375]},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Model Performance Metrics\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Precision vs Recall\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Confidence Distribution\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.375,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Class Performance\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.375,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"ğŸ¯ YOLOv8 Performance Dashboard\"},\"showlegend\":true,\"height\":800},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('080d3f86-9dc1-4541-8b2d-3e506be20a1b');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š DETAILED PERFORMANCE METRICS\n",
            "==================================================\n",
            "ğŸ¯ mAP@50: 0.563\n",
            "ğŸ¯ mAP@50-95: 0.421\n",
            "ğŸ“ˆ Precision: 0.603\n",
            "ğŸ“ˆ Recall: 0.504\n",
            "â° Timestamp: 2025-09-06T08:48:07.048572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RealTimeDetector:\n",
        "    def __init__(self, model_path='/content/best_model.pt'):\n",
        "        self.model = YOLO(model_path) if os.path.exists(model_path) else None\n",
        "        self.fps_history = []\n",
        "        self.detection_count = 0\n",
        "\n",
        "    def detect_image(self, image_path, conf_threshold=0.25):\n",
        "        \"\"\"Detect objects in single image\"\"\"\n",
        "        if not self.model:\n",
        "            return None, None\n",
        "\n",
        "        start_time = time.time()\n",
        "        results = self.model(image_path, conf=conf_threshold)\n",
        "        inference_time = time.time() - start_time\n",
        "\n",
        "        # Calculate FPS\n",
        "        fps = 1.0 / inference_time if inference_time > 0 else 0\n",
        "        self.fps_history.append(fps)\n",
        "\n",
        "        return results, fps\n",
        "\n",
        "    def process_results(self, results, original_image):\n",
        "        \"\"\"Process detection results and draw bounding boxes\"\"\"\n",
        "        if not results:\n",
        "            return original_image, []\n",
        "\n",
        "        detections = []\n",
        "        annotated_image = original_image.copy()\n",
        "\n",
        "        for r in results:\n",
        "            boxes = r.boxes\n",
        "            if boxes is not None:\n",
        "                for box in boxes:\n",
        "                    # Get box coordinates\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "                    confidence = box.conf[0].cpu().numpy()\n",
        "                    class_id = int(box.cls[0].cpu().numpy())\n",
        "\n",
        "                    # Get class name\n",
        "                    class_name = self.model.names[class_id]\n",
        "\n",
        "                    # Draw bounding box\n",
        "                    cv2.rectangle(annotated_image,\n",
        "                                (int(x1), int(y1)), (int(x2), int(y2)),\n",
        "                                (0, 255, 0), 2)\n",
        "\n",
        "                    # Draw label\n",
        "                    label = f\"{class_name}: {confidence:.2f}\"\n",
        "                    cv2.putText(annotated_image, label,\n",
        "                              (int(x1), int(y1) - 10),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "                    detections.append({\n",
        "                        'class': class_name,\n",
        "                        'confidence': float(confidence),\n",
        "                        'bbox': [int(x1), int(y1), int(x2), int(y2)]\n",
        "                    })\n",
        "\n",
        "        return annotated_image, detections\n",
        "\n",
        "    def create_inference_interface(self):\n",
        "        \"\"\"Create Gradio interface for real-time inference\"\"\"\n",
        "        def predict(image):\n",
        "            if image is None or not self.model:\n",
        "                return None, \"No image or model provided\"\n",
        "\n",
        "            # Convert PIL to numpy\n",
        "            image_np = np.array(image)\n",
        "\n",
        "            # Run detection\n",
        "            results, fps = self.detect_image(image)\n",
        "\n",
        "            # Process results\n",
        "            annotated_image, detections = self.process_results(results, image_np)\n",
        "\n",
        "            # Create detection summary\n",
        "            summary = f\"ğŸ¯ Detections: {len(detections)}\\n\"\n",
        "            summary += f\"âš¡ FPS: {fps:.1f}\\n\"\n",
        "            summary += f\"ğŸ“Š Average FPS: {np.mean(self.fps_history[-10:]):.1f}\\n\\n\"\n",
        "\n",
        "            for det in detections:\n",
        "                summary += f\"â€¢ {det['class']}: {det['confidence']:.2f}\\n\"\n",
        "\n",
        "            return annotated_image, summary\n",
        "\n",
        "        # Create Gradio interface\n",
        "        interface = gr.Interface(\n",
        "            fn=predict,\n",
        "            inputs=gr.Image(type=\"pil\"),\n",
        "            outputs=[\n",
        "                gr.Image(type=\"numpy\", label=\"Detections\"),\n",
        "                gr.Textbox(label=\"Detection Results\", lines=10)\n",
        "            ],\n",
        "            title=\"ğŸ¯ YOLOv8 Real-time Object Detection\",\n",
        "            description=\"Upload an image to detect objects in real-time!\",\n",
        "            theme=\"huggingface\"\n",
        "        )\n",
        "\n",
        "        return interface\n",
        "\n",
        "# Create real-time detector\n",
        "detector = RealTimeDetector()\n",
        "\n",
        "# Test on sample images\n",
        "test_images_path = '/content/yolo_dataset/test/images'\n",
        "if os.path.exists(test_images_path):\n",
        "    sample_images = os.listdir(test_images_path)[:5]\n",
        "\n",
        "    print(\"ğŸ” Testing detection on sample images...\")\n",
        "    for img_file in sample_images:\n",
        "        img_path = os.path.join(test_images_path, img_file)\n",
        "        results, fps = detector.detect_image(img_path)\n",
        "\n",
        "        if results:\n",
        "            img = cv2.imread(img_path)\n",
        "            annotated_img, detections = detector.process_results(results, img)\n",
        "\n",
        "            print(f\"ğŸ“· {img_file}: {len(detections)} objects detected, {fps:.1f} FPS\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ywfR3Xa_SbEs",
        "outputId": "725b5e58-2a9e-4150-a04d-d236f83e021d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Testing detection on sample images...\n",
            "\n",
            "image 1/1 /content/yolo_dataset/test/images/00072.jpg: 480x640 1 person, 1 giraffe, 45.0ms\n",
            "Speed: 1.1ms preprocess, 45.0ms inference, 5.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "ğŸ“· 00072.jpg: 2 objects detected, 7.9 FPS\n",
            "\n",
            "image 1/1 /content/yolo_dataset/test/images/00878.jpg: 480x640 2 tvs, 1 laptop, 2 mouses, 3 keyboards, 6.2ms\n",
            "Speed: 1.2ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "ğŸ“· 00878.jpg: 8 objects detected, 60.7 FPS\n",
            "\n",
            "image 1/1 /content/yolo_dataset/test/images/00665.jpg: 448x640 3 bottles, 1 toilet, 45.3ms\n",
            "Speed: 1.0ms preprocess, 45.3ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "ğŸ“· 00665.jpg: 4 objects detected, 18.0 FPS\n",
            "\n",
            "image 1/1 /content/yolo_dataset/test/images/00261.jpg: 480x640 1 bottle, 2 knifes, 4 pizzas, 2 chairs, 1 dining table, 6.8ms\n",
            "Speed: 1.2ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "ğŸ“· 00261.jpg: 10 objects detected, 56.6 FPS\n",
            "\n",
            "image 1/1 /content/yolo_dataset/test/images/00840.jpg: 640x448 1 stop sign, 44.8ms\n",
            "Speed: 1.1ms preprocess, 44.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
            "ğŸ“· 00840.jpg: 1 objects detected, 18.4 FPS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CompleteDashboard:\n",
        "    def __init__(self, model_path='/content/best_model.pt'):\n",
        "        self.detector = RealTimeDetector(model_path)\n",
        "        self.dashboard = MetricsDashboard(model_path)\n",
        "\n",
        "    def create_full_interface(self):\n",
        "        \"\"\"Create complete dashboard with multiple tabs\"\"\"\n",
        "\n",
        "        # Real-time detection interface\n",
        "        detection_interface = self.detector.create_inference_interface()\n",
        "\n",
        "        # Metrics visualization function\n",
        "        def show_metrics():\n",
        "            metrics = self.dashboard.calculate_metrics()\n",
        "            if metrics:\n",
        "                return f\"\"\"\n",
        "                ğŸ“Š **Current Model Performance**\n",
        "\n",
        "                ğŸ¯ **mAP@50:** {metrics['mAP50']:.3f}\n",
        "                ğŸ¯ **mAP@50-95:** {metrics['mAP50-95']:.3f}\n",
        "                ğŸ“ˆ **Precision:** {metrics['precision']:.3f}\n",
        "                ğŸ“ˆ **Recall:** {metrics['recall']:.3f}\n",
        "                â° **Last Updated:** {metrics['timestamp']}\n",
        "\n",
        "                ğŸ“Š **FPS Statistics**\n",
        "                ğŸš€ **Current FPS:** {self.detector.fps_history[-1] if self.detector.fps_history else 0:.1f}\n",
        "                ğŸ“ˆ **Average FPS:** {np.mean(self.detector.fps_history) if self.detector.fps_history else 0:.1f}\n",
        "                ğŸ“Š **Total Detections:** {self.detector.detection_count}\n",
        "                \"\"\"\n",
        "            return \"No metrics available\"\n",
        "\n",
        "        # Create tabbed interface\n",
        "        with gr.Blocks(theme=\"huggingface\", title=\"YOLOv8 Detection System\") as demo:\n",
        "            gr.Markdown(\"# ğŸ¯ YOLOv8 Custom Object Detection System\")\n",
        "            gr.Markdown(\"Complete end-to-end object detection with real-time inference and performance monitoring\")\n",
        "\n",
        "            with gr.Tabs():\n",
        "                with gr.TabItem(\"ğŸ” Real-time Detection\"):\n",
        "                    gr.Markdown(\"### Upload an image for object detection\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        with gr.Column():\n",
        "                            input_image = gr.Image(type=\"pil\", label=\"Input Image\")\n",
        "                            detect_btn = gr.Button(\"ğŸ¯ Detect Objects\", variant=\"primary\")\n",
        "\n",
        "                        with gr.Column():\n",
        "                            output_image = gr.Image(type=\"numpy\", label=\"Detection Results\")\n",
        "                            results_text = gr.Textbox(label=\"Detection Summary\", lines=10)\n",
        "\n",
        "                    detect_btn.click(\n",
        "                        self.detector.create_inference_interface().fn,\n",
        "                        inputs=[input_image],\n",
        "                        outputs=[output_image, results_text]\n",
        "                    )\n",
        "\n",
        "                with gr.TabItem(\"ğŸ“Š Performance Metrics\"):\n",
        "                    gr.Markdown(\"### Model Performance Dashboard\")\n",
        "\n",
        "                    metrics_display = gr.Markdown(show_metrics())\n",
        "                    refresh_btn = gr.Button(\"ğŸ”„ Refresh Metrics\", variant=\"secondary\")\n",
        "\n",
        "                    refresh_btn.click(\n",
        "                        lambda: show_metrics(),\n",
        "                        outputs=[metrics_display]\n",
        "                    )\n",
        "\n",
        "                with gr.TabItem(\"âš™ï¸ Model Info\"):\n",
        "                    gr.Markdown(f\"\"\"\n",
        "                    ### ğŸ¤– Model Information\n",
        "\n",
        "                    **Model Type:** YOLOv8 Nano\n",
        "                    **Training Dataset:** COCO 2017 (Custom Subset)\n",
        "                    **Classes:** {len(converter.class_names)} object categories\n",
        "                    **Input Size:** 640x640 pixels\n",
        "                    **Model Path:** `/content/best_model.pt`\n",
        "\n",
        "                    ### ğŸ“‹ Supported Classes:\n",
        "                    {', '.join(converter.class_names[:20])}{'...' if len(converter.class_names) > 20 else ''}\n",
        "\n",
        "                    ### ğŸ¯ Usage Instructions:\n",
        "                    1. Go to **Real-time Detection** tab\n",
        "                    2. Upload an image\n",
        "                    3. Click **Detect Objects**\n",
        "                    4. View results with bounding boxes\n",
        "                    5. Check **Performance Metrics** for model stats\n",
        "                    \"\"\")\n",
        "\n",
        "        return demo\n",
        "\n",
        "# Create and launch complete dashboard\n",
        "complete_dashboard = CompleteDashboard()\n",
        "dashboard_interface = complete_dashboard.create_full_interface()\n",
        "\n",
        "# Launch the interface\n",
        "print(\"ğŸš€ Launching YOLOv8 Detection Dashboard...\")\n",
        "dashboard_interface.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2710
        },
        "id": "jx02bX0qSe6M",
        "outputId": "fa166173-f0f5-4d9f-a8b8-e23c34fe1ccc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:\n",
            "\n",
            "\n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "\n",
            "/usr/local/lib/python3.12/dist-packages/gradio/blocks.py:1220: UserWarning:\n",
            "\n",
            "Cannot load huggingface. Caught Exception: 404 Client Error: Not Found for url: https://huggingface.co/api/spaces/huggingface (Request ID: Root=1-68bbf54a-1ec0f596442b5ae83fa09b30;fed3011b-8cf1-4531-a67b-1f17dbcf655c)\n",
            "\n",
            "Sorry, we can't find the page you are looking for.\n",
            "\n",
            "/usr/local/lib/python3.12/dist-packages/gradio/blocks.py:1220: UserWarning:\n",
            "\n",
            "Cannot load huggingface. Caught Exception: 404 Client Error: Not Found for url: https://huggingface.co/api/spaces/huggingface (Request ID: Root=1-68bbf54a-2e9b71c14e6e4f9476560134;cfc25d7c-6c37-4c34-a2ed-359985c36088)\n",
            "\n",
            "Sorry, we can't find the page you are looking for.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.194 ğŸš€ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/blocks.py:1220: UserWarning:\n",
            "\n",
            "Cannot load huggingface. Caught Exception: 404 Client Error: Not Found for url: https://huggingface.co/api/spaces/huggingface (Request ID: Root=1-68bbf54b-16d1bbc809fd0701765b0e30;4bc1a59b-9ee9-4278-93be-ec66225b72cf)\n",
            "\n",
            "Sorry, we can't find the page you are looking for.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2409.0Â±392.2 MB/s, size: 112.6 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/yolo_dataset/val/labels.cache... 200 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 200/200 352.2Kit/s 0.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 13/13 3.7it/s 3.5s\n",
            "                   all        200       1521      0.603      0.504      0.563      0.421\n",
            "                person        116        402      0.806      0.684      0.785       0.54\n",
            "               bicycle          6          6      0.744      0.488      0.694      0.318\n",
            "                   car         30         86      0.664      0.442      0.526      0.313\n",
            "            motorcycle          2          3      0.517      0.667      0.679      0.551\n",
            "              airplane          2          6      0.413      0.333      0.533      0.294\n",
            "                   bus          6          6      0.481      0.667      0.782      0.663\n",
            "                 train          6          6      0.915      0.833      0.931      0.842\n",
            "                 truck         13         23      0.589      0.304      0.458      0.341\n",
            "                  boat          3         14      0.799      0.214      0.329      0.105\n",
            "         traffic light          6         18      0.651      0.333      0.369      0.228\n",
            "          fire hydrant          6          7       0.88      0.714      0.768      0.605\n",
            "             stop sign          4          4      0.865          1      0.995      0.921\n",
            "         parking meter          1          1      0.293          1      0.995      0.995\n",
            "                 bench         12         28       0.77      0.107      0.137      0.117\n",
            "                  bird          4         21      0.551      0.238      0.371      0.221\n",
            "                   cat          6          6      0.551      0.667      0.636      0.564\n",
            "                   dog          2          2      0.823          1      0.995      0.829\n",
            "                 horse          4         10          1      0.624      0.788      0.479\n",
            "                 sheep          3         17      0.851      0.765      0.867      0.624\n",
            "                   cow          6         31      0.562      0.677      0.709      0.485\n",
            "              elephant          2          3      0.892          1      0.995      0.831\n",
            "                  bear          2          2      0.476        0.5      0.572      0.572\n",
            "                 zebra          2          3      0.765      0.667      0.913       0.79\n",
            "               giraffe          5         14      0.998          1      0.995       0.81\n",
            "              backpack         13         29      0.488      0.165      0.236      0.115\n",
            "              umbrella         10         18      0.531      0.667      0.682      0.436\n",
            "               handbag         18         32      0.658      0.219      0.283      0.146\n",
            "                   tie          9         17      0.852      0.341      0.516      0.263\n",
            "              suitcase          6         15      0.583      0.466      0.521      0.315\n",
            "               frisbee          3          3      0.796          1      0.995       0.83\n",
            "                  skis          8         12      0.633      0.433      0.481      0.293\n",
            "             snowboard          2          2      0.264        0.5      0.414      0.308\n",
            "           sports ball          8          9      0.548      0.556      0.548      0.474\n",
            "          baseball bat          7          9       0.48      0.333      0.475      0.234\n",
            "        baseball glove          7         13      0.787      0.569      0.653      0.456\n",
            "            skateboard          4          5      0.463        0.6      0.527      0.294\n",
            "             surfboard          6         21      0.642      0.429      0.389      0.225\n",
            "         tennis racket          3          7      0.498      0.286      0.514      0.372\n",
            "                bottle         19         35      0.715      0.501      0.547      0.382\n",
            "            wine glass          3          8       0.21      0.135       0.12     0.0276\n",
            "                   cup         13         43      0.526      0.326      0.391      0.261\n",
            "                  fork          5          9      0.693      0.556      0.602       0.46\n",
            "                 knife          9         23      0.358      0.087      0.169      0.111\n",
            "                 spoon          6          6          0          0      0.174      0.124\n",
            "                  bowl         10         17      0.406      0.412      0.441      0.358\n",
            "                banana          4          7      0.239      0.143      0.195      0.139\n",
            "                 apple          3         13      0.358      0.615       0.46      0.376\n",
            "              sandwich          3          6      0.817        0.5      0.601      0.524\n",
            "                orange          4          4      0.311        0.5      0.558      0.515\n",
            "              broccoli          2         18      0.252      0.151      0.212     0.0694\n",
            "                carrot          9         28      0.223      0.214      0.159     0.0933\n",
            "               hot dog          3          3          1          0      0.124     0.0876\n",
            "                 pizza          4          8      0.488      0.625      0.691      0.562\n",
            "                 donut          2          2      0.571        0.5      0.662      0.546\n",
            "                  cake          4         18      0.475      0.253      0.238      0.193\n",
            "                 chair         21         89       0.61      0.369      0.442      0.258\n",
            "                 couch          7         11      0.857      0.543      0.568      0.469\n",
            "          potted plant         10         22      0.651       0.34      0.456      0.325\n",
            "                   bed          7          7      0.491      0.714      0.677      0.554\n",
            "          dining table         22         32      0.484      0.382      0.476       0.33\n",
            "                toilet          3          3      0.688          1      0.995      0.951\n",
            "                    tv          9         12      0.475      0.667      0.755      0.553\n",
            "                laptop          9         20      0.536       0.55      0.547      0.429\n",
            "                 mouse          8          9      0.561      0.778      0.847      0.589\n",
            "                remote          4         12      0.381      0.208      0.338      0.162\n",
            "              keyboard          7         13      0.682      0.462      0.596      0.437\n",
            "            cell phone         10         11      0.588      0.727      0.656      0.532\n",
            "             microwave          4          4      0.514        0.5      0.716      0.472\n",
            "                  oven          4          4      0.499       0.75      0.746      0.696\n",
            "                  sink          9         12      0.436       0.25      0.268      0.159\n",
            "          refrigerator          7         23       0.58      0.261       0.42      0.253\n",
            "                  book          6         21      0.658      0.184       0.27      0.136\n",
            "                 clock          9         16      0.813      0.938      0.955      0.737\n",
            "                  vase          8         21      0.744      0.571      0.656      0.441\n",
            "              scissors          2          2      0.866          1      0.995      0.721\n",
            "            teddy bear          4         16      0.576     0.0909      0.131      0.111\n",
            "            hair drier          1          1          1          0          0          0\n",
            "            toothbrush          1          1      0.626          1      0.995      0.895\n",
            "Speed: 1.2ms preprocess, 3.8ms inference, 0.0ms loss, 3.0ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/val3\u001b[0m\n",
            "ğŸš€ Launching YOLOv8 Detection Dashboard...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://05d1734a59ddcd87b2.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://05d1734a59ddcd87b2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 448x640 1 fork, 1 carrot, 1 dining table, 47.8ms\n",
            "Speed: 1.6ms preprocess, 47.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 fork, 1 carrot, 1 dining table, 8.0ms\n",
            "Speed: 1.7ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://05d1734a59ddcd87b2.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_system_to_drive():\n",
        "    \"\"\"Save complete system to Google Drive\"\"\"\n",
        "    drive_save_path = '/content/drive/MyDrive/yolov8_detection_system'\n",
        "    os.makedirs(drive_save_path, exist_ok=True)\n",
        "\n",
        "    # Save model\n",
        "    if os.path.exists('/content/best_model.pt'):\n",
        "        shutil.copy2('/content/best_model.pt', f\"{drive_save_path}/best_model.pt\")\n",
        "\n",
        "    # Save training results\n",
        "    if os.path.exists('/content/training_results'):\n",
        "        shutil.copytree('/content/training_results', f\"{drive_save_path}/training_results\", dirs_exist_ok=True)\n",
        "\n",
        "    # Save dataset info\n",
        "    if os.path.exists('/content/yolo_dataset/dataset.yaml'):\n",
        "        shutil.copy2('/content/yolo_dataset/dataset.yaml', f\"{drive_save_path}/dataset.yaml\")\n",
        "\n",
        "    # Create system info\n",
        "    system_info = {\n",
        "        'model_type': 'YOLOv8n',\n",
        "        'classes': len(converter.class_names),\n",
        "        'class_names': converter.class_names,\n",
        "        'training_epochs': 30,\n",
        "        'created': datetime.now().isoformat(),\n",
        "        'performance': dashboard.metrics_history[-1] if dashboard.metrics_history else None\n",
        "    }\n",
        "\n",
        "    with open(f\"{drive_save_path}/system_info.json\", 'w') as f:\n",
        "        json.dump(system_info, f, indent=2)\n",
        "\n",
        "    print(f\"âœ… System saved to Google Drive: {drive_save_path}\")\n",
        "    return drive_save_path\n",
        "\n",
        "# Save system\n",
        "saved_path = save_system_to_drive()"
      ],
      "metadata": {
        "id": "ZNPLhLdLSjIw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d2c7ecc7-03d7-4b64-c01d-24adcd501d5b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… System saved to Google Drive: /content/drive/MyDrive/yolov8_detection_system\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ‰ YOLOV8 DETECTION SYSTEM COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\"\"\n",
        "ğŸš€ **System Components Successfully Built:**\n",
        "\n",
        "ğŸ“Š **Dataset:** COCO 2017 converted to YOLO format\n",
        "   - Training images: {len(os.listdir('/content/yolo_dataset/train/images')) if os.path.exists('/content/yolo_dataset/train/images') else 0}\n",
        "   - Validation images: {len(os.listdir('/content/yolo_dataset/val/images')) if os.path.exists('/content/yolo_dataset/val/images') else 0}\n",
        "   - Test images: {len(os.listdir('/content/yolo_dataset/test/images')) if os.path.exists('/content/yolo_dataset/test/images') else 0}\n",
        "\n",
        "ğŸ¤– **Model:** YOLOv8 Nano trained on custom dataset\n",
        "   - Classes: {len(converter.class_names)}\n",
        "   - Model saved: {'/content/best_model.pt' if os.path.exists('/content/best_model.pt') else 'Not found'}\n",
        "\n",
        "ğŸ“Š **Performance Metrics:**\n",
        "   - mAP@50: {f\"{dashboard.metrics_history[-1]['mAP50']:.3f}\" if dashboard.metrics_history else 'N/A'}\n",
        "   - mAP@50-95: {f\"{dashboard.metrics_history[-1]['mAP50-95']:.3f}\" if dashboard.metrics_history else 'N/A'}\n",
        "   - Precision: {f\"{dashboard.metrics_history[-1]['precision']:.3f}\" if dashboard.metrics_history else 'N/A'}\n",
        "   - Recall: {f\"{dashboard.metrics_history[-1]['recall']:.3f}\" if dashboard.metrics_history else 'N/A'}\n",
        "\n",
        "ğŸ¯ **Real-time Detection:** Gradio interface with live inference\n",
        "ğŸ”„ **Dashboard:** Interactive performance monitoring\n",
        "ğŸ’¾ **Saved to:** {saved_path}\n",
        "\n",
        "ğŸ›ï¸ **Dashboard Features:**\n",
        "   âœ… Real-time object detection\n",
        "   âœ… Performance metrics visualization\n",
        "   âœ… FPS monitoring\n",
        "   âœ… Interactive web interface\n",
        "   âœ… Model information display\n",
        "\n",
        "ğŸš€ **Ready for Production Use!**\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nğŸ”— The Gradio interface is running above - use it to test detection!\")\n",
        "print(\"ğŸ“Š Switch between tabs to explore different features.\")\n",
        "print(\"ğŸ’¾ All components are saved to Google Drive for future use.\")"
      ],
      "metadata": {
        "id": "zgqrJl93Smjg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "51bf94de-4d5a-465a-a7c3-c8e1c6b6db37"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸ‰ YOLOV8 DETECTION SYSTEM COMPLETE!\n",
            "============================================================\n",
            "\n",
            "ğŸš€ **System Components Successfully Built:**\n",
            "\n",
            "ğŸ“Š **Dataset:** COCO 2017 converted to YOLO format\n",
            "   - Training images: 700\n",
            "   - Validation images: 200\n",
            "   - Test images: 100\n",
            "\n",
            "ğŸ¤– **Model:** YOLOv8 Nano trained on custom dataset\n",
            "   - Classes: 80\n",
            "   - Model saved: /content/best_model.pt\n",
            "\n",
            "ğŸ“Š **Performance Metrics:**\n",
            "   - mAP@50: 0.563\n",
            "   - mAP@50-95: 0.421\n",
            "   - Precision: 0.603\n",
            "   - Recall: 0.504\n",
            "\n",
            "ğŸ¯ **Real-time Detection:** Gradio interface with live inference\n",
            "ğŸ”„ **Dashboard:** Interactive performance monitoring\n",
            "ğŸ’¾ **Saved to:** /content/drive/MyDrive/yolov8_detection_system\n",
            "\n",
            "ğŸ›ï¸ **Dashboard Features:**\n",
            "   âœ… Real-time object detection\n",
            "   âœ… Performance metrics visualization\n",
            "   âœ… FPS monitoring\n",
            "   âœ… Interactive web interface\n",
            "   âœ… Model information display\n",
            "\n",
            "ğŸš€ **Ready for Production Use!**\n",
            "\n",
            "\n",
            "ğŸ”— The Gradio interface is running above - use it to test detection!\n",
            "ğŸ“Š Switch between tabs to explore different features.\n",
            "ğŸ’¾ All components are saved to Google Drive for future use.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZTdKELJlSrJT"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}